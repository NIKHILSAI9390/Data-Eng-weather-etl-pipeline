# ğŸŒ¦ï¸ Weather ETL Pipeline  
### ğŸš€ Production-Style Data Engineering Project

---

## ğŸš€ Project Overview

This project is a **production-style ETL pipeline** built using **Apache Airflow** that:

- Extracts real-time weather data from OpenWeatherMap API  
- Validates schema and data quality  
- Transforms and converts units  
- Stores partitioned Parquet files in AWS S3  
- Supports dev/prod environments  
- Implements error handling, retries, and email alerts  

This project demonstrates real-world data engineering practices including:

- Airflow DAG orchestration  
- Cloud storage (AWS S3)  
- Schema validation  
- Data quality checks  
- Partitioned Parquet storage  
- Environment-based configuration  
- Failure alerting & retries  

---

## ğŸ— Architecture Diagram

            +-------------------+
            | OpenWeatherMap API|
            +-------------------+
                      |
                      v
            +-------------------+
            |  Airflow DAG      |
            |-------------------|
            | 1. HttpSensor     |
            | 2. Extract Task   |
            | 3. Transform Task |
            +-------------------+
                      |
                      v
            +-------------------+
            | Schema Validation |
            | + Data Quality    |
            +-------------------+
                      |
                      v
            +-------------------+
            | Transform + Clean |
            +-------------------+
                      |
                      v
            +-------------------+
            | Partitioned       |
            | Parquet Files     |
            | + Email Alerts    |
            +-------------------+
                      |
                      v
            +-------------------+
            | AWS S3 Bucket     |
            | (dev / prod)      |
            +-------------------+

---

## ğŸ“¦ Tech Stack

- Python 3.10  
- Apache Airflow  
- Pandas  
- PyArrow  
- AWS S3  
- s3fs  
- OpenWeatherMap API  

---

## ğŸ” ETL Flow

### 1ï¸âƒ£ Extract
- Uses **HttpSensor** to check API availability  
- Uses **SimpleHttpOperator** to fetch JSON weather data  
- City is parameterized  

---

### 2ï¸âƒ£ Transform
- Convert Kelvin to Fahrenheit  
- Extract relevant weather fields  
- Create structured Pandas DataFrame  

---

### 3ï¸âƒ£ Schema Validation & Data Quality

Validates:

- Required JSON keys  
- Required `main` fields  
- Temperature realistic range (200Kâ€“350K)  
- Humidity range (0â€“100)  
- Wind speed non-negative  

---

### 4ï¸âƒ£ Load

- Stores **Partitioned Parquet** in S3  
- Partitioned by execution date  
- Environment-based bucket selection  

---

## ğŸ—‚ S3 Storage Structure


---

## âœ¨ Additional Features Implemented

### ğŸ“§ Email Alerts
- Email notifications configured for task failures  
- Alerts triggered automatically on retry exhaustion  
- Uses Airflowâ€™s built-in email configuration  
- Enables production-grade monitoring  

---

### ğŸŒ Parameterized City
- Supports dynamic city configuration  
- Controlled via environment variables or Airflow Variables  

---

### ğŸŒ Environment Configuration
- Bucket selection dynamically changes based on environment  
- Supports:
  - `weather-etl-dev-nikhil`
  - `weather-etl-nikhil`

---

## ğŸ§  Resume Value

This project demonstrates:

âœ” Apache Airflow orchestration  
âœ” REST API integration  
âœ” Schema validation & data quality checks  
âœ” Partitioned Parquet data lake design  
âœ” AWS S3 cloud integration  
âœ” Environment-based configuration (dev/prod)  
âœ” Parameterized pipelines  
âœ” Email alerting & retry strategy  
âœ” Production-style ETL architecture  

---

## ğŸ“Œ Future Improvements (Next Level)

- Add CI/CD for DAG deployment  
- Add unit tests for transform function  
- Integrate Athena for querying Parquet files  
- Add data catalog integration  
- Add monitoring dashboard  

---
